{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K-Mx8ukmZON"
      },
      "source": [
        "## Interacting with Azure OpenAI Service\n",
        "\n",
        "Finally, we will look at how to interact with the Azure OpenAI service to perform various tasks related to our code."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==1.13.3"
      ],
      "metadata": {
        "id": "rhzA2QmInOpt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57b664f1-11d8-40cc-8599-719be790a1fe"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==1.13.3\n",
            "  Downloading openai-1.13.3-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.13.3) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.13.3) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai==1.13.3)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.13.3) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai==1.13.3) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.13.3) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai==1.13.3) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.13.3) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.13.3) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.13.3) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai==1.13.3)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.13.3)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.13.3) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.13.3) (2.20.1)\n",
            "Downloading openai-1.13.3-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 openai-1.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from openai import AsyncAzureOpenAI"
      ],
      "metadata": {
        "id": "2nQMKZ8znVLW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "code_language = \"Python\"\n",
        "user_story = input(\"user story\")\n",
        "acceptance_criteria = input(\"acceptance criteria\")"
      ],
      "metadata": {
        "id": "PcE1WeqQduNh",
        "outputId": "adc48dbe-4499-410c-e222-d45f674cb9f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "user storyAs a user, I want a secure login page to access my account and manage my information.\n",
            "acceptance criteriaUsers can enter their username and password to log in. A \"Forgot Password\" link is available for password recovery. A \"Remember Me\" checkbox is present for persistent login. A link to the registration page is available for new users. Clear error messages are displayed for invalid login attempts. The login page is responsive across desktops, tablets, and smartphones. Passwords are encrypted, and security measures are in place to protect against brute-force attacks.     General Note : To predict the accuracy of a neural network model, you first need to understand that for regression tasks (like predicting laptop prices), accuracy isn't typically used as a metric, since it's designed for classification tasks. Instead, for regression tasks, you would use metrics like Mean Squared Error (MSE), Mean Absolute Error (MAE), or R-squared (R²).   Explanation of Metrics: Mean Squared Error (MSE): Measures the average squared difference between predicted and actual values. Lower is better. Formula:MSE=1n∑i=1n(y^i−yi)2\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2MSE=n1i=1∑n(y^i−yi)2 Mean Absolute Error (MAE): Measures the average absolute difference between predicted and actual values. Lower is better. Formula:MAE=1n∑i=1n∣y^i−yi∣\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |\\hat{y}_i - y_i|MAE=n1i=1∑n∣y^i−yi∣ R-squared (R²): R² is a statistical measure that represents how close the data are to the fitted regression line. The value ranges from 0 to 1, where 1 means perfect predictions. Formula:R2=1−∑(y^i−yi)2∑(yi−yˉ)2R^2 = 1 - \\frac{\\sum (\\hat{y}_i - y_i)^2}{\\sum (y_i - \\bar{y})^2}R2=1−∑(yi−yˉ)2∑(y^i−yi)2 It tells you how well your model explains the variance in the data. How to Interpret: MSE and MAE: Lower values indicate better performance. R²: Closer to 1 means the model explains most of the variance in the target (good fit). Closer to 0 means the model has poor predictive power.   a7634543e2b0475ead78083a834406fc   swedencentral   https://eygroup2.openai.azure.com/   EYopenAIGroup2   generate code programming language scenario/problem generate comments   {code_language}  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qMm1E3pmeU-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: generate the prompt for below scenario \"User inputs the user story description and acceptance criteria . options to generate code in multiple programming languages. ( pythn, C##). \"\n",
        "\n",
        "System_prompt = \"\"\"You are a helpful code generation assistant.\n",
        "The user will provide a user story description and acceptance criteria.\n",
        "Based on the provided input, generate code that satisfies the user's requirements.\n",
        "The user may also specify the desired programming language, such as Python or C#.\n",
        "generated output should be only the python code and dont inlcude any other messages\n",
        "or comments\n",
        "\"\"\"\n",
        "\n",
        "user_prompt = f\"\"\"\n",
        "Generate code for the following user story in {code_language}:\n",
        "User Story: {user_story}.\n",
        "Acceptance Criteria:{acceptance_criteria}\n",
        "\n",
        "Further, also generate unit test cases and provide comments for the code generated.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "hLdTmULrU_3B"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JGVF7ZoqVlEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmPGJhzpmZOP"
      },
      "source": [
        "\n",
        "\n",
        "# Set to True to print the full response from OpenAI for each call\n",
        "printFullResponse = False\n",
        "\n",
        "async def main():\n",
        "  try:\n",
        "        # Get configuration settings\n",
        "        # Configuration settings\n",
        "        azure_oai_endpoint = \"https://eygroup2.openai.azure.com/\"\n",
        "        azure_oai_key = \"a7634543e2b0475ead78083a834406fc\"\n",
        "        azure_oai_deployment = \"EYopenAIGroup2\"\n",
        "\n",
        "        # Configure the Azure OpenAI client\n",
        "        client = AsyncAzureOpenAI(\n",
        "            azure_endpoint = azure_oai_endpoint,\n",
        "            api_key=azure_oai_key,\n",
        "            api_version=\"2024-02-15-preview\"\n",
        "        )\n",
        "        await call_openai_model(user_prompt, model=azure_oai_deployment, client=client)\n",
        "\n",
        "  except Exception as ex:\n",
        "        print(ex)\n",
        "\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def call_openai_model(prompt, model, client):\n",
        "    # Provide a basic user message, and use the prompt content as the user message\n",
        "    system_message = System_prompt\n",
        "    user_message = prompt\n",
        "\n",
        "    # Format and send the request to the model\n",
        "    messages =[\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": user_message},\n",
        "    ]\n",
        "\n",
        "    # Call the Azure OpenAI model\n",
        "    response = await client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0.7,\n",
        "        max_tokens=4000\n",
        "    )\n",
        "\n",
        "    # Print the response to the console, if desired\n",
        "    if printFullResponse:\n",
        "        print(response)\n",
        "\n",
        "    # Write the response to a file\n",
        "    results_file = open(file=\"app.py\", mode=\"w\", encoding=\"utf8\")\n",
        "    results_file.write(response.choices[0].message.content)\n",
        "    print(\"\\nResponse written to result/app.py\\n\\n\")\n"
      ],
      "metadata": {
        "id": "t-DWkZ0ImvrZ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the main function in the event loop\n",
        "await main()"
      ],
      "metadata": {
        "id": "tDoRyK1Am3In",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31d5f8f0-05f1-4990-b41e-108c5efdca7f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Response written to result/app.py\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gu6OuWHmdcyI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}